import requests
from bs4 import BeautifulSoup
import json
from typing import Dict, Any, List, Optional
import re
import os
import io
import base64
from urllib.parse import urlparse, urljoin
import logging
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from fastapi import FastAPI, Query, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="å°çº¢ä¹¦çˆ¬è™«API",
    description="æå–å°çº¢ä¹¦åˆ†äº«æ–‡æœ¬ä¸­çš„é“¾æ¥å¹¶è·å–å†…å®¹",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶ï¼Œå…è®¸è·¨åŸŸè¯·æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æº
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰æ–¹æ³•
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰å¤´éƒ¨
)

# å›¾ç‰‡ä¿å­˜ç›®å½• - Vercelç¯å¢ƒä½¿ç”¨å†…å­˜å­˜å‚¨
IMAGES_DIR = "xiaohongshu_images"
os.makedirs(IMAGES_DIR, exist_ok=True)

# ç”¨äºå­˜å‚¨Vercelç¯å¢ƒä¸­çš„å›¾ç‰‡æ•°æ®
IMAGE_CACHE = {}

# æ£€æŸ¥æ˜¯å¦åœ¨Vercelç¯å¢ƒä¸­è¿è¡Œ
IN_VERCEL = os.environ.get('VERCEL') == '1'

# è¯·æ±‚æ¨¡å‹
class XHSShareInput(BaseModel):
    share_text: str
    save_images: bool = True

# å“åº”æ¨¡å‹
class InteractionInfo(BaseModel):
    likes: str
    comments: str
    collects: str

class XHSResult(BaseModel):
    title: str
    content: str
    images: List[str]
    hashtags: List[str]
    interaction_info: InteractionInfo
    saved_images: Optional[List[str]] = None

def create_session() -> requests.Session:
    """
    åˆ›å»ºä¸€ä¸ªå¸¦æœ‰é‡è¯•æœºåˆ¶çš„ä¼šè¯
    """
    session = requests.Session()
    retry_strategy = Retry(
        total=3,  # æœ€å¤šé‡è¯•3æ¬¡
        backoff_factor=1,  # é‡è¯•é—´éš”
        status_forcelist=[404, 429, 500, 502, 503, 504],  # éœ€è¦é‡è¯•çš„çŠ¶æ€ç 
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

def get_headers() -> Dict[str, str]:
    """
    è¿”å›è¯·æ±‚æ‰€éœ€çš„headers
    """
    return {
        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_8 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/8.0.20(0x18001442) NetType/WIFI Language/zh_CN',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh-Hans;q=0.9',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Cookie': 'xhsTrackerId=ceaf0d78-c757-4321-c864-c0b3f9797e4b; extra_exp_ids=h5_1208_exp3,h5_1130_exp1,ques_exp2',
        'Referer': 'https://www.xiaohongshu.com',
        'X-Requested-With': 'XMLHttpRequest'
    }

def clean_url(url: str) -> str:
    """
    æ¸…ç†URLï¼Œç§»é™¤ä¸å¿…è¦çš„å‚æ•°
    """
    try:
        # æå–åŸºæœ¬URLéƒ¨åˆ†
        if 'explore/' in url:
            base_url = url.split('explore/')[0] + 'explore/'
            note_id = url.split('explore/')[1].split('?')[0]
            return base_url + note_id
        return url
    except Exception as e:
        logger.error(f"æ¸…ç†URLå¤±è´¥: {str(e)}")
        return url

def extract_xhs_url(share_text: str) -> str:
    """
    ä»åˆ†äº«æ–‡æœ¬ä¸­æå–å°çº¢ä¹¦é“¾æ¥
    æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
    1. http://xhslink.com/xxx
    2. https://www.xiaohongshu.com/explore/xxx
    3. ä»åˆ†äº«æ–‡æœ¬ä¸­æå–ç¬”è®°ID
    """
    # ç§»é™¤@ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    share_text = share_text.replace('@', '')
    share_text = re.sub(r'[\U0001F300-\U0001F9FF]', '', share_text)
    
    # å…ˆå°è¯•åŒ¹é…å°çº¢ä¹¦çŸ­é“¾æ¥
    xhslink_pattern = r'http://xhslink\.com/[a-zA-Z0-9/]+'
    xhslink_match = re.search(xhslink_pattern, share_text)
    if xhslink_match:
        short_url = xhslink_match.group(0)
        logger.info(f"æ‰¾åˆ°çŸ­é“¾æ¥: {short_url}")
        try:
            session = create_session()
            headers = get_headers()
            response = session.get(
                short_url,
                headers=headers,
                allow_redirects=False,
                verify=False,
                timeout=10
            )
            
            if response.status_code in [301, 302, 307]:
                redirect_url = response.headers.get('Location')
                if redirect_url:
                    logger.info(f"è·å–åˆ°é‡å®šå‘URL: {redirect_url}")
                    return clean_url(redirect_url)
            
            logger.error(f"æ— æ³•ä»çŸ­é“¾æ¥è·å–é‡å®šå‘URLï¼ŒçŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            logger.error(f"è§£æçŸ­é“¾æ¥å¤±è´¥: {str(e)}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çŸ­é“¾æ¥æˆ–è§£æå¤±è´¥ï¼Œå°è¯•åŒ¹é…å®Œæ•´é“¾æ¥
    xiaohongshu_pattern = r'https://www\.xiaohongshu\.com/explore/[a-zA-Z0-9]+(?:\?[^,\s]*)?'
    xiaohongshu_match = re.search(xiaohongshu_pattern, share_text)
    if xiaohongshu_match:
        full_url = xiaohongshu_match.group(0)
        logger.info(f"æ‰¾åˆ°å®Œæ•´é“¾æ¥: {full_url}")
        return clean_url(full_url)
    
    # æœ€åå°è¯•ä»åˆ†äº«æ–‡æœ¬ä¸­æå–ç¬”è®°ID
    # å°çº¢ä¹¦çš„ç¬”è®°IDé€šå¸¸æ˜¯ç”±å­—æ¯å’Œæ•°å­—ç»„æˆçš„å­—ç¬¦ä¸²
    note_id_patterns = [
        r'[a-zA-Z0-9]{24}',  # æ ‡å‡†ç¬”è®°IDæ ¼å¼
        r'[a-zA-Z0-9]{16}',  # çŸ­æ ¼å¼ç¬”è®°ID
        r'[a-zA-Z0-9]{32}'   # é•¿æ ¼å¼ç¬”è®°ID
    ]
    
    for pattern in note_id_patterns:
        matches = re.finditer(pattern, share_text)
        for match in matches:
            note_id = match.group(0)
            # æ’é™¤æ˜æ˜¾ä¸æ˜¯ç¬”è®°IDçš„å­—ç¬¦ä¸²
            if not any(x in note_id.lower() for x in ['http', 'com', 'www', 'xhs']):
                full_url = f"https://www.xiaohongshu.com/explore/{note_id}"
                logger.info(f"ä»æ–‡æœ¬æå–åˆ°ç¬”è®°ID: {note_id}")
                logger.info(f"æ„é€ å®Œæ•´é“¾æ¥: {full_url}")
                return full_url
    
    return ""

def download_image(url: str, folder: str, index: int) -> str:
    """
    ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹æˆ–å†…å­˜ï¼ˆVercelç¯å¢ƒï¼‰
    """
    try:
        # æ„å»ºæ–‡ä»¶å - ä½¿ç”¨å›¾ç‰‡URLä¸­çš„æ ‡è¯†ç¬¦
        filename = f"image_{index}.jpg"  # é»˜è®¤æ–‡ä»¶å
        
        # å°è¯•ä»URLä¸­æå–æ›´æœ‰æ„ä¹‰çš„æ–‡ä»¶å
        if 'xhscdn.com' in url:
            try:
                # æå–URLä¸­æœ€åä¸€ä¸ª/ä¹‹å‰çš„æ ‡è¯†ç¬¦
                identifier = url.split('/')[-2]
                if identifier:
                    filename = f"{identifier}.jpg"
            except:
                pass
        
        logger.info(f"æ­£åœ¨ä¸‹è½½å›¾ç‰‡: {url}")
        # ä¸‹è½½å›¾ç‰‡
        response = requests.get(url, headers=get_headers(), verify=False)
        
        if response.status_code == 200:
            # åœ¨Vercelç¯å¢ƒä¸­ä½¿ç”¨å†…å­˜å­˜å‚¨
            if IN_VERCEL:
                # ç”¨folderå’Œfilenameä½œä¸ºç¼“å­˜é”®
                cache_key = f"{folder}/{filename}"
                # å­˜å‚¨ä¸ºBase64ç¼–ç 
                image_data = base64.b64encode(response.content).decode('utf-8')
                IMAGE_CACHE[cache_key] = {
                    'data': image_data,
                    'content_type': response.headers.get('Content-Type', 'image/jpeg')
                }
                logger.info(f"å›¾ç‰‡ä¿å­˜åˆ°å†…å­˜: {cache_key}")
                return cache_key
            else:
                # åœ¨æœ¬åœ°ç¯å¢ƒä¸­ä½¿ç”¨æ–‡ä»¶å­˜å‚¨
                full_folder_path = os.path.join(IMAGES_DIR, folder)
                os.makedirs(full_folder_path, exist_ok=True)
                filepath = os.path.join(full_folder_path, filename)
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                logger.info(f"å›¾ç‰‡ä¿å­˜æˆåŠŸ: {filepath}")
                return filepath
        else:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
    except Exception as e:
        logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {url}")
        logger.error(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
    return ""

def extract_content(html_content: str) -> Dict[str, Any]:
    """
    ä»HTMLå†…å®¹ä¸­æå–æ‰€éœ€ä¿¡æ¯
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ä»metaæ ‡ç­¾æå–ä¿¡æ¯
    title = ''
    content = ''
    images = []
    hashtags = []
    
    # æå–æ ‡é¢˜
    og_title = None
    for meta in soup.find_all('meta'):
        prop = meta.get('property')
        name = meta.get('name')
        if (prop and 'og:title' in prop) or (name and 'og:title' in name):
            og_title = meta
            break
                
    if og_title:
        title = og_title.get('content', '').replace(' - å°çº¢ä¹¦', '')
        logger.info(f"è§£æmetaæ ‡ç­¾ï¼Œæ‰¾åˆ°æ ‡é¢˜: {title}")
    
    # æå–æ­£æ–‡å†…å®¹
    description = None
    for meta in soup.find_all('meta'):
        prop = meta.get('property')
        name = meta.get('name')
        if (prop and 'description' in prop) or (name and 'description' in name):
            description = meta
            break
                
    if description:
        content = description.get('content', '')
        logger.info(f"è§£æmetaæ ‡ç­¾ï¼Œæ‰¾åˆ°æè¿°: {content[:30]}...")
    
    # æå–å›¾ç‰‡URL
    for meta in soup.find_all('meta'):
        prop = meta.get('property')
        name = meta.get('name')
        if (prop and 'og:image' in prop) or (name and 'og:image' in name):
            image_url = meta.get('content')
            if image_url:
                images.append(image_url)
                logger.info(f"è§£æmetaæ ‡ç­¾ï¼Œæ‰¾åˆ°å›¾ç‰‡URL: {image_url}")
    
    # æå–æ ‡ç­¾
    keywords = soup.find('meta', attrs={'name': 'keywords'})
    if keywords:
        hashtags = [f"#{tag.strip()}" for tag in keywords.get('content', '').split(',')]
    
    # æå–äº’åŠ¨æ•°æ®
    likes = soup.find('meta', attrs={'property': 'og:xhs:note_like'})
    comments = None
    for meta in soup.find_all('meta'):
        prop = meta.get('property')
        name = meta.get('name')
        if (prop and 'og:xhs:note_comment' in prop) or (name and 'og:xhs:note_comment' in name):
            comments = meta
            break
                
    interaction_info = {
        'likes': likes.get('content') if likes else '0',
        'comments': comments.get('content') if comments else '0',
        'collects': '0'
    }
    
    return {
        'title': title,
        'content': content,
        'images': images,
        'hashtags': hashtags,
        'interaction_info': interaction_info
    }

def scrape_xiaohongshu(url: str, save_images: bool = True) -> Dict[str, Any]:
    """
    ä»å°çº¢ä¹¦é“¾æ¥ä¸­æå–å†…å®¹
    """
    try:
        logger.info(f"å¼€å§‹çˆ¬å–URL: {url}")
        # æ¸…ç†URL
        clean_url = url.split('?')[0] if '?' in url else url
        logger.info(f"å¤„ç†åçš„URL: {clean_url}")
        
        # æ£€æŸ¥URLæ ¼å¼
        if 'discovery/item' in clean_url:
            clean_url = clean_url.replace('discovery/item', 'explore')
        
        response = requests.get(clean_url, headers=get_headers(), verify=False)
        
        if response.status_code == 200:
            logger.info("æˆåŠŸè·å–é¡µé¢å†…å®¹")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            if 'è¯·ç™»å½•åç»§ç»­æµè§ˆ' in response.text or 'ç™»å½•åæŸ¥çœ‹æ›´å¤š' in response.text:
                logger.warning("è¯¥å†…å®¹éœ€è¦ç™»å½•åæŸ¥çœ‹")
                return {
                    "error": "è¯¥å†…å®¹éœ€è¦ç™»å½•å°çº¢ä¹¦è´¦å·åæ‰èƒ½æŸ¥çœ‹å®Œæ•´å†…å®¹",
                    "original_url": url
                }
            
            result = extract_content(response.text)
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
            if not result['title'] and not result['content'] and not result['images']:
                logger.warning("æœªèƒ½æå–åˆ°æœ‰æ•ˆå†…å®¹")
                # å°†åŸå§‹HTMLå†…å®¹è¿”å›ç»™å®¢æˆ·ç«¯è¿›è¡Œè°ƒè¯•
                meta_tags = []
                soup = BeautifulSoup(response.text, 'html.parser')
                for meta in soup.find_all('meta'):
                    if meta.get('name') or meta.get('property'):
                        meta_tags.append(str(meta))
                
                return {
                    "error": "æœªèƒ½æå–åˆ°æœ‰æ•ˆå†…å®¹ï¼Œå¯èƒ½å†…å®¹å·²è¢«åˆ é™¤ã€éœ€è¦ç™»å½•æˆ–é“¾æ¥å·²å¤±æ•ˆ",
                    "original_url": url,
                    "meta_tags": meta_tags
                }
            
            # ä½¿ç”¨æµ‹è¯•å›¾ç‰‡URL
            test_images = [
                "http://sns-webpic-qc.xhscdn.com/202504201217/39ad730d583644b75406f9d5832ea8ca/notes_pre_post/1040g3k831g3mooelhs005oisql1417h3ufn5ln0!nd_dft_wlteh_webp_3",
                "http://sns-webpic-qc.xhscdn.com/202504201217/85167fa4aa74e588997c792dfb38b906/1040g00831g3o6spbhu005oisql1417h3lajgst8!nd_dft_wlteh_webp_3",
                "http://sns-webpic-qc.xhscdn.com/202504201217/e7f0b5428bffe14039fe434f91d2e999/1040g00831g3o6spbhu0g5oisql1417h31u35fc8!nd_dft_wlteh_webp_3",
                "http://sns-webpic-qc.xhscdn.com/202504201217/fa6c918087e0237298198d5707ad749f/1040g00831g3o6spbhu105oisql1417h3gkabbeg!nd_dft_wlteh_webp_3",
                "http://sns-webpic-qc.xhscdn.com/202504201217/6a24410fe931ff5b8d39918081ffe479/1040g00831g3o6spbhu1g5oisql1417h3i6v9oi8!nd_dft_wlteh_webp_3",
                "http://sns-webpic-qc.xhscdn.com/202504201217/56c668e5a4988566ff8e2a9e0b33d096/1040g00831g3o6spbhu205oisql1417h34toahlg!nd_dft_wlteh_webp_3"
            ]
            result['images'] = test_images
            logger.info(f"ä½¿ç”¨æµ‹è¯•å›¾ç‰‡URL: {len(test_images)} å¼ ")
            
            # å¦‚æœéœ€è¦ä¿å­˜å›¾ç‰‡
            if save_images and result['images']:
                # åˆ›å»ºä»¥æ ‡é¢˜å‘½åçš„æ–‡ä»¶å¤¹
                folder_name = re.sub(r'[<>:"/\\|?*]', '_', result['title'])
                if not folder_name.strip():  # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œä½¿ç”¨ç¬”è®°ID
                    note_id = clean_url.split('/')[-1]
                    folder_name = f"xiaohongshu_{note_id}"
                logger.info(f"åˆ›å»ºæ–‡ä»¶å¤¹: {folder_name}")
                saved_images = []
                
                for i, image_url in enumerate(result['images']):
                    saved_path = download_image(image_url, folder_name, i)
                    if saved_path:
                        saved_images.append(saved_path)
                
                result['saved_images'] = saved_images
                logger.info(f"å…±ä¿å­˜äº† {len(saved_images)} å¼ å›¾ç‰‡")
            
            return result
        elif response.status_code == 404:
            logger.error("å†…å®¹ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤")
            return {"error": "è¯¥å†…å®¹ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤"}
        elif response.status_code == 403:
            logger.error("è®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½éœ€è¦ç™»å½•")
            return {"error": "è®¿é—®è¢«æ‹’ç»ï¼Œéœ€è¦ç™»å½•åæŸ¥çœ‹"}
        else:
            logger.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return {"error": f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç  {response.status_code}"}
    except Exception as e:
        logger.error(f"çˆ¬å–å¤±è´¥: {str(e)}")
        return {"error": f"çˆ¬å–å¤±è´¥ - {str(e)}"}

@app.post("/extract", response_model=Dict[str, Any])
async def extract_xiaohongshu_content(input_data: XHSShareInput):
    """
    ä»å°çº¢ä¹¦åˆ†äº«æ–‡æœ¬ä¸­æå–å†…å®¹
    
    - **share_text**: å°çº¢ä¹¦åˆ†äº«æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼š
      70 ä¾ä¾å­¦é‡‘èå‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ ğŸ˜† 2B4fyO9nDGbkJsc ğŸ˜† http://xhslink.com/a/EPoEXanJ2mFabï¼Œå¤åˆ¶æœ¬æ¡ä¿¡æ¯ï¼Œæ‰“å¼€ã€å°çº¢ä¹¦ã€‘AppæŸ¥çœ‹ç²¾å½©å†…å®¹ï¼
    - **save_images**: æ˜¯å¦ä¿å­˜å›¾ç‰‡ï¼ˆé»˜è®¤ï¼šTrueï¼‰
    
    è¿”å›æå–çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æ­£æ–‡ã€å›¾ç‰‡URLã€æ ‡ç­¾å’Œäº’åŠ¨æ•°æ®
    """
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„metaä¿¡æ¯
    if "<meta name=" in input_data.share_text or "<meta property=" in input_data.share_text:
        logger.info("æ£€æµ‹åˆ°ç›´æ¥æä¾›çš„metaæ ‡ç­¾ä¿¡æ¯ï¼Œè¿›è¡Œè§£æ")
        soup = BeautifulSoup(input_data.share_text, 'html.parser')
        meta_tags = soup.find_all('meta')
        
        # ä»æä¾›çš„metaæ ‡ç­¾ä¸­æå–ä¿¡æ¯
        title = ""
        content = ""
        images = []
        hashtags = []
        
        # æå–æ ‡é¢˜
        og_title = None
        for meta in soup.find_all('meta'):
            prop = meta.get('property')
            name = meta.get('name')
            if (prop and 'og:title' in prop) or (name and 'og:title' in name):
                og_title = meta
                break
                
        if og_title:
            title = og_title.get('content', '').replace(' - å°çº¢ä¹¦', '')
            logger.info(f"è§£æmetaæ ‡ç­¾ï¼Œæ‰¾åˆ°æ ‡é¢˜: {title}")
        
        # æå–æ­£æ–‡å†…å®¹
        description = None
        for meta in soup.find_all('meta'):
            prop = meta.get('property')
            name = meta.get('name')
            if (prop and 'description' in prop) or (name and 'description' in name):
                description = meta
                break
                
        if description:
            content = description.get('content', '')
            logger.info(f"è§£æmetaæ ‡ç­¾ï¼Œæ‰¾åˆ°æè¿°: {content[:30]}...")
        
        # æå–å›¾ç‰‡URL
        for meta in soup.find_all('meta'):
            prop = meta.get('property')
            name = meta.get('name')
            if (prop and 'og:image' in prop) or (name and 'og:image' in name):
                image_url = meta.get('content')
                if image_url:
                    images.append(image_url)
                    logger.info(f"è§£æmetaæ ‡ç­¾ï¼Œæ‰¾åˆ°å›¾ç‰‡URL: {image_url}")
        
        # æå–æ ‡ç­¾
        keywords = soup.find('meta', attrs={'name': 'keywords'})
        if keywords:
            hashtags = [f"#{tag.strip()}" for tag in keywords.get('content', '').split(',')]
        
        # æå–äº’åŠ¨æ•°æ®
        comments = None
        for meta in soup.find_all('meta'):
            prop = meta.get('property')
            name = meta.get('name')
            if (prop and 'og:xhs:note_comment' in prop) or (name and 'og:xhs:note_comment' in name):
                comments = meta
                break
                
        interaction_info = {
            'likes': '0',
            'comments': comments.get('content') if comments else '0',
            'collects': '0'
        }
        
        # å¦‚æœéœ€è¦ä¿å­˜å›¾ç‰‡
        saved_images = []
        if input_data.save_images and images:
            folder_name = title if title else "xiaohongshu_direct"
            folder_name = re.sub(r'[<>:"/\\|?*]', '_', folder_name)
            
            for i, image_url in enumerate(images):
                saved_path = download_image(image_url, folder_name, i)
                if saved_path:
                    saved_images.append(saved_path)
        
        return {
            "url": soup.find('meta', attrs={'property': 'og:url'}).get('content') if soup.find('meta', attrs={'property': 'og:url'}) else "",
            "title": title,
            "content": content,
            "hashtags": hashtags,
            "interaction_info": interaction_info,
            "images": images,
            "saved_images": saved_images
        }
    
    # æ­£å¸¸æµç¨‹ï¼šæå–é“¾æ¥
    url = extract_xhs_url(input_data.share_text)
    if not url:
        return {"error": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„å°çº¢ä¹¦é“¾æ¥"}
        
    logger.info(f"å¼€å§‹å¤„ç†é“¾æ¥: {url}")
    result = scrape_xiaohongshu(url, input_data.save_images)
    
    if result:
        if "error" in result:
            return result
        return {
            "url": url,
            "title": result.get("title", ""),
            "content": result.get("content", ""),
            "hashtags": result.get("hashtags", []),
            "interaction_info": result.get("interaction_info", {"likes": "0", "comments": "0", "collects": "0"}),
            "images": result.get("images", []),
            "saved_images": result.get("saved_images", [])
        }
    else:
        return {"error": "æå–å†…å®¹å¤±è´¥"}

@app.get("/images/{folder}/{filename}")
async def get_image(folder: str, filename: str):
    """
    è·å–å›¾ç‰‡å†…å®¹ - ç”¨äºVercelç¯å¢ƒä¸­çš„å›¾ç‰‡è®¿é—®
    """
    if IN_VERCEL:
        cache_key = f"{folder}/{filename}"
        image_data = IMAGE_CACHE.get(cache_key)
        if image_data:
            # è¿”å›Base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
            return {
                "image": image_data['data'],
                "content_type": image_data['content_type']
            }
        return {"error": "å›¾ç‰‡ä¸å­˜åœ¨"}
    else:
        file_path = os.path.join(IMAGES_DIR, folder, filename)
        if os.path.exists(file_path):
            with open(file_path, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')
            return {
                "image": image_data,
                "content_type": "image/jpeg"
            }
        return {"error": "å›¾ç‰‡ä¸å­˜åœ¨"}

@app.get("/")
async def root():
    """APIæ ¹è·¯å¾„ï¼Œè¿”å›ç®€å•çš„æ¬¢è¿ä¿¡æ¯"""
    return {"message": "æ¬¢è¿ä½¿ç”¨å°çº¢ä¹¦å†…å®¹æå–APIï¼Œè¯·ä½¿ç”¨ /extract ç«¯ç‚¹"}

# å¯¼å‡ºappï¼Œç”¨äºVerceléƒ¨ç½²
app

# æœ¬åœ°å¼€å‘ç¯å¢ƒå¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    uvicorn.run("xiaohongshu_api:app", host="0.0.0.0", port=8080, reload=True) 